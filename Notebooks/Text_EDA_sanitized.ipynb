{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; display:block\">\n",
    "    <div style=\"display: inline-block\">\n",
    "        <h1  style=\"text-align: center\">Text EDA Module</h1>\n",
    "        <div style=\"width:80%; text-align: center\"><i>Author:</i> <strong>Soham Mullick</strong> </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of This module is to perform EDA on the processed text data output from the previous text cleaner module to gain insights on the classification problem. To better understand the diffrence between the features of the two different class - for our case RMA and No RMA the same EDA steps are performed for each of the class examples\n",
    "\n",
    "<b>Input</b> - The clean file from the text cleaner module\n",
    "\n",
    "<b>Output</b> - TextStat calculation results (mainly containing count, Log_freq, TF-Idf etc.) for both RMA and No RMA cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Core modules\n",
    "from collections import Counter\n",
    "import operator\n",
    "import math as m\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import logging\n",
    "import time\n",
    "\n",
    "#Gensim modules for text processing\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFile(fileName):\n",
    "    try :\n",
    "        raw_data=pd.read_csv(fileName,encoding='latin-1') #Change the Filename in config to use different Dataset\n",
    "    except FileNotFoundError:\n",
    "        print('\\n File name not Correct. Please try again')\n",
    "        raw_data=getFile(fileName)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text EDA Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used for doing the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count the number of occurances\n",
    "def count_words(list_of_doc):\n",
    "    count=Counter()\n",
    "    for document in list_of_doc:\n",
    "        document=str(document)\n",
    "        count.update(document.split())\n",
    "    return count\n",
    "\n",
    "# Count Number of documents having a word\n",
    "def doc_count(word,docs):\n",
    "    count=0\n",
    "    docs=list(docs)\n",
    "    for i in range(len(docs)):\n",
    "        if word in str(docs[i]).split():\n",
    "            count+=1\n",
    "            continue\n",
    "    return count    \n",
    "\n",
    "# Log of Word Count\n",
    "def wordCountlog(Wordcount):\n",
    "    wordCountDictLog = {}\n",
    "    for key, val in Wordcount.items():\n",
    "        wordCountDictLog[key] = 1 + m.log(float(val))\n",
    "    return wordCountDictLog.values()\n",
    "\n",
    "# Augmented frequency (Word Count Normalised by max count)\n",
    "def Augfreq(Wordcount):\n",
    "    augmentedFrequency = {}\n",
    "    maxfreq = max(list(Wordcount.values()))\n",
    "    for key, val in Wordcount.items():\n",
    "        augmentedFrequency[key] = 0.5 + 0.5 * (val/maxfreq)\n",
    "    return augmentedFrequency.values()\n",
    "\n",
    "# Count Inverse Document Frequency\n",
    "def invDocFreq(Wordcount,docs):\n",
    "    N = len(docs)\n",
    "    wordCountDictLog = {}\n",
    "    for key, val in Wordcount.items():\n",
    "        num_docs_having_the_word = 1+doc_count(key,docs)\n",
    "        wordCountDictLog[key] =m.log(1 + N/num_docs_having_the_word)\n",
    "    return wordCountDictLog.values()\n",
    "\n",
    "\n",
    "#To Get all the relevant Text statistics in a table\n",
    "def TextStat(wc_dict,docs,fileName,saveFile=False):\n",
    "    log_freq=wordCountlog(wc_dict)\n",
    "    aug_freq=Augfreq(wc_dict)\n",
    "    idf=invDocFreq(wc_dict,docs)\n",
    "    TextDataframe = pd.DataFrame(index=wc_dict.keys())\n",
    "    TextDataframe['Counts']= (wc_dict.values())\n",
    "    TextDataframe['Term Frequency']=TextDataframe['Counts']/len(TextDataframe)\n",
    "    TextDataframe['Log Frequency']= (log_freq)\n",
    "    TextDataframe['Augmented Frequency']= (aug_freq)\n",
    "    TextDataframe['Inverse Document Term Frequency']= idf\n",
    "    TextDataframe['TF-IDF'] = TextDataframe['Term Frequency'] * TextDataframe['Inverse Document Term Frequency']\n",
    "    \n",
    "    ### CIF (Classification Importance Factor is a derived metric to normalize between important words for different classes.\n",
    "    ### The assumption behind this being that the word occuring with similar importance in both the classes do not help much \n",
    "    ### in the classification decision between the classes)\n",
    "    \n",
    "    TextDataframe['CIF'] = TextDataframe['Augmented Frequency']*(1/TextDataframe['Inverse Document Term Frequency'])\n",
    "    if saveFile:\n",
    "        TextDataframe.to_csv(fileName,index=True)\n",
    "    return TextDataframe\n",
    "\n",
    "# To create dictionary and filter based on count(for differential treatment of RMA and NO RMA cases)\n",
    "def count_filter(textList):\n",
    "    dic_count=count_words(textList)\n",
    "    sorted_dic=sorted(dic_count.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    min_count=EDA_min_count\n",
    "    output_dict={k: v for k, v in dict(sorted_dic).items() if ((v > min_count))}\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Config and create logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./config.ini')\n",
    "\n",
    "# Read config file\n",
    "input_file=str(config['Text_EDA']['Input_file'])\n",
    "datacol_ug=str(config['Text_EDA']['Datacol_UG'])\n",
    "datacol_bg=str(config['Text_EDA']['Datacol_BG'])\n",
    "rma_ug_output=str(config['Text_EDA']['RMA_UG_TextStat'])\n",
    "rma_bg_output=str(config['Text_EDA']['RMA_BG_TextStat'])\n",
    "norma_ug_output=str(config['Text_EDA']['NoRMA_UG_TextStat'])\n",
    "norma_bg_output=str(config['Text_EDA']['NoRMA_BG_TextStat'])\n",
    "\n",
    "common_list = config['Text_processing']['added_stop_list']\n",
    "common_list = list(common_list.replace(' ', '').split(','))\n",
    "\n",
    "# Create logger file\n",
    "logging.basicConfig(filename=\"Text_EDA_{}.log\".format(time.strftime('%b-%d-%Y_%H%M',time.localtime())),level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data=getFile(input_file)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Info about the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.debug('Total No. of cases in clean data '+str(len(clean_data)))\n",
    "logging.debug('Total No. of RMA cases in clean data '+str(len(clean_data[clean_data['rma_flag']==0])))\n",
    "logging.debug('Total No. of NO RMA cases in clean data '+str(len(clean_data[clean_data['rma_flag']==1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data on Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the dataset based on RMA and No RMA cases\n",
    "rma_data=clean_data[clean_data['rma_flag']==0]\n",
    "norma_data=clean_data[clean_data['rma_flag']==1]\n",
    "logging.debug('In the RMA dataset no. of cases are '+str(len(rma_data)))\n",
    "logging.debug('In the No RMA dataset no. of cases are '+str(len(norma_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply EDA steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole analysis is divided into two different aspect:\n",
    "\n",
    "    To analyse the distribution of words between classes (RMA and NO RMA)\n",
    "    To analyse how the bigrams and unigrams are acting differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations on RMA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Control-Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EDA_min_count=20           #To filter based on count - RMA Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uni-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rma_dict=count_filter(rma_data[datacol_ug])\n",
    "RMADataframeUG=TextStat(rma_dict,rma_data[datacol_ug],rma_ug_output,saveFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.debug(\"The total number of words in RMA UG dictionary: \"+str(len(rma_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bi-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rma_bigram_dict=count_filter(rma_data[datacol_bg])\n",
    "RMADataframeBG=TextStat(rma_bigram_dict,rma_data[datacol_bg],rma_bg_output,saveFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.debug(\"The total number of words in RMA BG dictionary: \"+str(len(rma_bigram_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations on No RMA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Control-Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EDA_min_count=30   #To filter based on count - No RMA Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uni-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norma_dict=count_filter(norma_data[datacol_ug])\n",
    "NoRMADataframeUG=TextStat(norma_dict,norma_data[datacol_ug],norma_ug_output,saveFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.debug(\"The total number of words in No RMA UG dictionary: \"+str(len(norma_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bi-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norma_bigram_dict=count_filter(norma_data[datacol_bg])\n",
    "NoRMADataframeBG=TextStat(norma_bigram_dict,norma_data[datacol_bg],norma_bg_output,saveFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.debug(\"The total number of words in No RMA BG dictionary: \"+str(len(norma_bigram_dict)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
