{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; display:block\">\n",
    "    <div style=\"display: inline-block\">\n",
    "        <h1  style=\"text-align: center\">Word Embedding Module</h1>\n",
    "        <div style=\"width:80%; text-align: center\"><i>Author:</i> <strong>Soham Mullick</strong> </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this module is to generate custom word embedding vector from the cleaned and processed text data to be used for Final Prediction Model code\n",
    "\n",
    "<b>Input</b> - Cleaned Data output from Text_Cleaner module\n",
    "\n",
    "<b>Output</b> - Trained word2vec embedding object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words. Words that show up in similar contexts, such as \"black\", \"white\", and \"red\" will have vectors near each other. There are two architectures for implementing word2vec, CBOW (Continuous Bag-Of-Words) and Skip-gram. \n",
    "\n",
    "A word embedding is a way of representing text where each word in the vocabulary is represented by a real valued vector in a high-dimensional space. The vectors are learned in such a way that words that have similar meanings will have similar representation in the vector space (close in the vector space). This is a more expressive representation for text than more classical methods like bag-of-words, where relationships between words or tokens are ignored, or forced in bigram and trigram approaches.\n",
    "  \n",
    "In this implementation, we'll be using the skip-gram architecture because it performs better than CBOW. Here, we pass in a word and try to predict the words surrounding it in the text. In this way, we can train the network to learn representations for words that show up in similar contexts.\n",
    "\n",
    "We also use gensim python library to create and store embeddings. The python code for word2vec implementation is part of Word2VecClass() instantiated inside Models.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Core modules\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import logging\n",
    "import time\n",
    "\n",
    "#Gensim modules\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "#Visualisation modules\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Control Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters can be used to tweak the training for word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size=300      #Dimensionality of the feature vectors\n",
    "window_size=10           #Maximum distance between the current and predicted word within a sentence\n",
    "learning_rate=0.025     #Initial learning rate\n",
    "use_sg=1                #Using Skip-Gram Algorithm (Set 0 for using CBOW)\n",
    "negative_sampling=5    #Negative sampling will be used, the int for negative specifies how many “noise words” should be drawn\n",
    "iter_no=150             #Number of iterations (epochs) over the corpus \n",
    "min_count=20             #Ignore all words with total frequency lower than this\n",
    "workers=4               #How many worker threads to train the model(Only for multi-core machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used by the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To Get Data\n",
    "\n",
    "def getFile(fileName):\n",
    "    try :\n",
    "        raw_data=pd.read_csv(fileName,encoding='latin-1') #Change the Filename to use different Dataset\n",
    "    except FileNotFoundError:\n",
    "        print('\\n File name not Correct. Please try again')\n",
    "        raw_data=getFile()\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "# Using iterable class to be used by word2vec\n",
    "class Sentence(object):\n",
    "    def __init__(self, doc_list):\n",
    "        self.doc_list = list(doc_list)\n",
    "    def __iter__(self):\n",
    "        for doc in self.doc_list:\n",
    "            yield str(doc).split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Config and Create Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./config.ini')\n",
    "\n",
    "# Read config file\n",
    "textData = str(config['Word_Embedding']['Data_col'])\n",
    "input_file=str(config['Word_Embedding']['Input_file'])\n",
    "output_file=str(config['Word_Embedding']['Output_file'])\n",
    "\n",
    "# Create logger file\n",
    "logging.basicConfig(filename=\"Word_Embedding_{}.log\".format(time.strftime('%b-%d-%Y_%H%M',time.localtime())),level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CleanRawData=getFile(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Whole corpus to train the word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decide on Corpus to be used for training\n",
    "data=CleanRawData[textData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an instance of the iterable class\n",
    "sentences=Sentence(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Word2Vec Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Model is being trained on the individual sentence generated out of the sentence generator which in turn takes the whole column as mentioned in the config file and creates individual sentence objects. \n",
    "\n",
    "The vocabulary is also made on the go. There are different variants to train the wordEmbedding on a pre-defined vocabulary.\n",
    "\n",
    "The associated parameters to be tweaked for varying the training pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model=Word2Vec(sentences,size=embedding_size, window=window_size,alpha=learning_rate,sg=use_sg,negative=negative_sampling,iter=iter_no,workers=4,min_count=min_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the Final Vocabulary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.debug(\"The total number of words in the current Word2Vec Model: \"+str(len(w2v_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for most similar word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somullic\\AppData\\Local\\Continuum\\anaconda3_new\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('stack', 0.584490180015564),\n",
       " ('power_supply', 0.41783031821250916),\n",
       " ('tried', 0.3626179099082947),\n",
       " ('one', 0.3552532196044922),\n",
       " ('two', 0.35395902395248413),\n",
       " ('stacked', 0.3537466526031494),\n",
       " ('showing', 0.34573429822921753),\n",
       " ('powered', 0.3440735936164856),\n",
       " ('cable', 0.3430076539516449),\n",
       " ('issue', 0.3364446759223938)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " w2v_model.most_similar(positive=[\"switch\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use T-SNE for dimension reduction and try to visualise the whole Word Embedding Vector space in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TSNE Model\n",
    "'''Can Take a lot of time to Run. Make Sure to Comment it out once TSNE Model is Created'''\n",
    "def tsne_cal(model):\n",
    "    \"Creates and TSNE model\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    values = tsne_model.fit_transform(tokens)\n",
    "    return values\n",
    "\n",
    "def tsne_plot_bokeh(tsne_values,model):\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in tsne_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    source_train = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = x,\n",
    "            y = y,\n",
    "            desc = list(model.wv.vocab)\n",
    "    )\n",
    "    )\n",
    "\n",
    "    hover_tsne = HoverTool(names=['train'],tooltips=[(\"Word\", \"@desc\")])\n",
    "    tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "    plot_tsne = figure(plot_width=600, plot_height=600, tools=tools_tsne, title='Embeddings')\n",
    "    plot_tsne.circle('x', 'y', size=10, #fill_color='colors', \n",
    "                     alpha=0.5, line_width=0, source=source_train, name=\"train\")\n",
    "\n",
    "    show(plot_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projecting Word Vectors in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_values=tsne_cal(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_plot_bokeh(tsne_values,w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving The Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving the word2vec model to be used later\n",
    "w2v_model.save(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
